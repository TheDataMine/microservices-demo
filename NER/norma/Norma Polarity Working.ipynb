{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon Polarity\n",
    "## Norma Grubb and Justin Gould\n",
    "## April 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentence_polarity\n",
    "import random\n",
    "import Subjectivity\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentence_polarity to\n",
      "[nltk_data]     /Users/gould29/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/sentence_polarity.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('sentence_polarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the sentence corpus and look at sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10662\n",
      "<class 'nltk.corpus.reader.util.ConcatenatedCorpusView'>\n",
      "['neg', 'pos']\n",
      "['simplistic', ',', 'silly', 'and', 'tedious', '.']\n",
      "[\"it's\", 'so', 'laddish', 'and', 'juvenile', ',', 'only', 'teenage', 'boys', 'could', 'possibly', 'find', 'it', 'funny', '.']\n",
      "['exploitative', 'and', 'largely', 'devoid', 'of', 'the', 'depth', 'or', 'sophistication', 'that', 'would', 'make', 'watching', 'such', 'a', 'graphic', 'treatment', 'of', 'the', 'crimes', 'bearable', '.']\n",
      "['[garbus]', 'discards', 'the', 'potential', 'for', 'pathological', 'study', ',', 'exhuming', 'instead', ',', 'the', 'skewed', 'melodrama', 'of', 'the', 'circumstantial', 'situation', '.']\n"
     ]
    }
   ],
   "source": [
    "# get the sentence corpus and look at some sentences\n",
    "sentences = sentence_polarity.sents()\n",
    "print(len(sentences))\n",
    "print(type(sentences))\n",
    "print(sentence_polarity.categories())\n",
    "# sentences are already tokenized, print the first four sentences\n",
    "for sent in sentences[:4]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "5331\n"
     ]
    }
   ],
   "source": [
    "# look at the sentences by category to see how many positive and negative\n",
    "pos_sents = sentence_polarity.sents(categories='pos')\n",
    "print(len(pos_sents))\n",
    "neg_sents = sentence_polarity.sents(categories='neg')\n",
    "print(len(neg_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup the movie reviews sentences for classification\n",
    "# create a list of documents, each document is one sentence as a list of words paired with category\n",
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "\tfor sent in sentence_polarity.sents(categories=cat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['simplistic', ',', 'silly', 'and', 'tedious', '.'], 'neg')\n",
      "(['provides', 'a', 'porthole', 'into', 'that', 'noble', ',', 'trembling', 'incoherence', 'that', 'defines', 'us', 'all', '.'], 'pos')\n"
     ]
    }
   ],
   "source": [
    "# look at the first and last documents - consists of all the words in the review\n",
    "# followed by the category\n",
    "print(documents[0])\n",
    "print(documents[-1])\n",
    "# randomly reorder documents\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get all words from all movie_reviews and put into a frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'the', ',', 'a', 'and', 'of', 'to', 'is', 'in', 'that', 'it', 'as', 'but', 'with', 'film', 'this', 'for', 'its', 'an', 'movie', \"it's\", 'be', 'on', 'you', 'not', 'by', 'about', 'more', 'one', 'like', 'has', 'are', 'at', 'from', 'than', '\"', 'all', '--', 'his', 'have', 'so', 'if', 'or', 'story', 'i', 'too', 'just', 'who', 'into', 'what']\n"
     ]
    }
   ],
   "source": [
    "#   note lowercase, but no stemming or stopwords\n",
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "# get the 2000 most frequently appearing keywords in the corpus\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word,count) in word_items]\n",
    "print(word_features[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define features (keywords) of a document for a BOW/unigram baseline each feature is 'contains(keyword)' and is true or false depending on whether that keyword is in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features sets for a document, including keyword features and category feature\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in documents]\n",
    "\n",
    "# the feature sets are 2000 words long so you may not want to look at one\n",
    "# featuresets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Na√Øve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training using naive Baysian classifier, training set is approximately 90% of data\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the accuracy of the classifier\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                V_boring = True              neg : pos    =     30.3 : 1.0\n",
      "             V_wonderful = True              pos : neg    =     21.7 : 1.0\n",
      "            V_engrossing = True              pos : neg    =     20.4 : 1.0\n",
      "              V_powerful = True              pos : neg    =     17.8 : 1.0\n",
      "               V_generic = True              neg : pos    =     15.0 : 1.0\n",
      "                  V_dull = True              neg : pos    =     14.5 : 1.0\n",
      "               V_routine = True              neg : pos    =     14.3 : 1.0\n",
      "              V_supposed = True              neg : pos    =     14.3 : 1.0\n",
      "            V_refreshing = True              pos : neg    =     13.7 : 1.0\n",
      "                  V_flat = True              neg : pos    =     13.4 : 1.0\n",
      "                    V_90 = True              neg : pos    =     11.7 : 1.0\n",
      "                 V_stale = True              neg : pos    =     11.7 : 1.0\n",
      "             V_inventive = True              pos : neg    =     11.0 : 1.0\n",
      "          V_refreshingly = True              pos : neg    =     11.0 : 1.0\n",
      "              V_mindless = True              neg : pos    =     11.0 : 1.0\n",
      "                V_stupid = True              neg : pos    =     11.0 : 1.0\n",
      "              V_provides = True              pos : neg    =     10.6 : 1.0\n",
      "                  V_warm = True              pos : neg    =     10.6 : 1.0\n",
      "             V_realistic = True              pos : neg    =     10.3 : 1.0\n",
      "                   V_car = True              neg : pos    =     10.3 : 1.0\n",
      "                V_dreary = True              neg : pos    =     10.3 : 1.0\n",
      "           V_pretentious = True              neg : pos    =     10.1 : 1.0\n",
      "              V_captures = True              pos : neg    =      9.8 : 1.0\n",
      "                  V_loud = True              neg : pos    =      9.8 : 1.0\n",
      "                  V_ages = True              pos : neg    =      9.7 : 1.0\n",
      "           V_mesmerizing = True              pos : neg    =      9.7 : 1.0\n",
      "                 V_empty = True              neg : pos    =      9.7 : 1.0\n",
      "             V_offensive = True              neg : pos    =      9.7 : 1.0\n",
      "              V_annoying = True              neg : pos    =      9.0 : 1.0\n",
      "            V_apparently = True              neg : pos    =      9.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# show which features of classifier are most informative\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Provided tff Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your own path to the subjclues file\n",
    "SLpath = \"./subjclueslen1-HLTEMNLP05.tff\"\n",
    "\n",
    "SL = Subjectivity.readSubjectivity(SLpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6885"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words are in the dictionary\n",
    "len(SL.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['strongsubj', 'adj', False, 'neutral']\n",
      "['strongsubj', 'adj', False, 'negative']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dog'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ad8b7cc890b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shabby'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# note what happens if the word is not there\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dog'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'dog'"
     ]
    }
   ],
   "source": [
    "# look at words in the dictionary\n",
    "print(SL['absolute'])\n",
    "print(SL['shabby'])\n",
    "# note what happens if the word is not there\n",
    "print(SL['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "# use multiple assignment to get the 4 items\n",
    "strength, posTag, isStemmed, polarity = SL['absolute']\n",
    "print(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features that include word counts of subjectivity words\n",
    "# negative feature will have number of weakly negative words +\n",
    "#    2 * number of strongly negative words\n",
    "# positive feature has similar definition\n",
    "#    not counting neutral words\n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_featuresets = [(SL_features(d, word_features, SL), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['unofficially', ',', 'national', \"lampoon's\", 'van', 'wilder', 'is', 'son', 'of', 'animal', 'house', '.', 'officially', ',', 'it', 'is', 'twice', 'as', 'bestial', 'but', 'half', 'as', 'funny', '.'], 'neg')\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# show document 0 and just the two sentiment lexicon features \n",
    "print(documents[0])\n",
    "print(SL_featuresets[0][0]['positivecount'])\n",
    "print(SL_featuresets[0][0]['negativecount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n",
      "2002\n"
     ]
    }
   ],
   "source": [
    "# this gives the label of document 0\n",
    "print(SL_featuresets[0][1])\n",
    "# number of features for document 0\n",
    "print(len(SL_featuresets[0][0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.748"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrain the classifier using these features\n",
    "train_set, test_set = SL_featuresets[1000:], SL_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'a', 'difference', 'between', 'movies', 'with', 'the', 'courage', 'to', 'go', 'over', 'the', 'top', 'and', 'movies', 'that', \"don't\", 'care', 'about', 'being', 'stupid']\n",
      "['a', 'farce', 'of', 'a', 'parody', 'of', 'a', 'comedy', 'of', 'a', 'premise', ',', 'it', \"isn't\", 'a', 'comparison', 'to', 'reality', 'so', 'much', 'as', 'it', 'is', 'a', 'commentary', 'about', 'our', 'knowledge', 'of', 'films', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['i', \"didn't\", 'laugh', '.', 'i', \"didn't\", 'smile', '.', 'i', 'survived', '.']\n",
      "['most', 'of', 'the', 'problems', 'with', 'the', 'film', \"don't\", 'derive', 'from', 'the', 'screenplay', ',', 'but', 'rather', 'the', 'mediocre', 'performances', 'by', 'most', 'of', 'the', 'actors', 'involved']\n",
      "['the', 'lack', 'of', 'naturalness', 'makes', 'everything', 'seem', 'self-consciously', 'poetic', 'and', 'forced', '.', '.', '.', \"it's\", 'a', 'pity', 'that', \"[nelson's]\", 'achievement', \"doesn't\", 'match', 'his', 'ambition', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in list(sentences)[:50]:\n",
    "    for word in sent:\n",
    "        if (word.endswith(\"n't\")):\n",
    "            print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this list of negation words includes some \"approximate negators\" like hardly and rarely\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One strategy with negation words is to negate the word following the negation word\n",
    "#   other strategies negate all words up to the next punctuation\n",
    "# Strategy is to go through the document words in order adding the word features,\n",
    "#   but if the word follows a negation words, change the feature to negated word\n",
    "# Start the feature set with all 2000 word features and 2000 Not word features set to false\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# define the feature sets\n",
    "NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in documents]\n",
    "# show the values of a couple of example features\n",
    "print(NOT_featuresets[0][0]['V_NOTcare'])\n",
    "print(NOT_featuresets[0][0]['V_always'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.782"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularize and Generalize Code - Gould"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"\n",
    "SRT010G900 overlap with 0305900SRT0807E00 overlap with 0305900SRT0706Z00 \\\n",
    "overlap with 0305900SRT0807E00 overlap with 010G900SRT0706Z00 overlap with \\\n",
    "010G900steam cleaned engine added dye and ran truck at high idle found gear \\\n",
    "cover leaking removed hood and bumper drained coolant recovered Freon removed \\\n",
    "coolant reservoir, ps reservoir, both radiator support, upper and lower rad hoses, \\\n",
    "radiator, ac compressor and bracket, alternator, fan, fan shroud, fan hub, removed \\\n",
    "and resealed gear cover reinstalled all removed parts refilled coolant and Freon ran \\\n",
    "truck at high idle no leaks repair completeOIL LEAK EXTERNALUPPER GEAR COVER GASKETLEAKS \\\n",
    "EPR Part Number:430716600 OIL1045962 THURSDAY 31OCT2019 05:00:47 AM\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "doc = tokenizer(sample.lower()) #NOTE LOWERCASE!!\n",
    "tokens = [word.text for word in doc]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Frequency Distribution of all Words in Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_JG = nltk.FreqDist(tokens)\n",
    "word_items_JG = all_words_JG.most_common(2000)\n",
    "word_features_JG = [word for (word,count) in word_items_JG]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SL Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "SL_featuresets = [SL_features(tokens, word_features_JG, SL)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide Supplemental Negation Words and Negate Any Words in Doc After a Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_featuresets = [NOT_features(tokens, word_features_JG, negationwords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_inference(featuresets):\n",
    "    for fs in featuresets:\n",
    "        internal = []\n",
    "        for key in fs:\n",
    "            internal.append({key : fs[key]})\n",
    "    \n",
    "    return internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens 66 \n",
      "\n",
      "[{'V_,': True}, {'V_and': True}, {'V_overlap': True}, {'V_with': True}, {'V_removed': True}, {'V_gear': True}, {'V_cover': True}, {'V_coolant': True}, {'V_fan': True}, {'V_\\n': True}, {'V_0305900srt0807e00': True}, {'V_ran': True}, {'V_truck': True}, {'V_at': True}, {'V_high': True}, {'V_idle': True}, {'V_freon': True}, {'V_reservoir': True}, {'V_radiator': True}, {'V_srt010g900': True}, {'V_0305900srt0706z00': True}, {'V_010g900srt0706z00': True}, {'V_010g900steam': True}, {'V_cleaned': True}, {'V_engine': True}, {'V_added': True}, {'V_dye': True}, {'V_found': True}, {'V_leaking': True}, {'V_hood': True}, {'V_bumper': True}, {'V_drained': True}, {'V_recovered': True}, {'V_ps': True}, {'V_both': True}, {'V_support': True}, {'V_upper': True}, {'V_lower': True}, {'V_rad': True}, {'V_hoses': True}, {'V_ac': True}, {'V_compressor': True}, {'V_bracket': True}, {'V_alternator': True}, {'V_shroud': True}, {'V_hub': True}, {'V_resealed': True}, {'V_reinstalled': True}, {'V_all': True}, {'V_parts': True}, {'V_refilled': True}, {'V_no': True}, {'V_leaks': True}, {'V_repair': True}, {'V_completeoil': True}, {'V_leak': True}, {'V_externalupper': True}, {'V_gasketleaks': True}, {'V_epr': True}, {'V_part': True}, {'V_number:430716600': True}, {'V_oil1045962': True}, {'V_thursday': True}, {'V_31oct2019': True}, {'V_05:00:47': True}, {'V_am': True}]\n"
     ]
    }
   ],
   "source": [
    "internal = prep_inference(SL_featuresets)[:-2]\n",
    "print(\"Num tokens\", len(internal), \"\\n\")\n",
    "print(internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use NLTK Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos', 'pos', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg']\n"
     ]
    }
   ],
   "source": [
    "preds = classifier.classify_many(internal)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Output\n",
    "Attach polarity to tokens...mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_formatting(preds, internal):\n",
    "    output = {}\n",
    "    negatives = []\n",
    "    final = {}\n",
    "    for pred, tok in zip(preds, internal):\n",
    "        word = list(tok.keys())[0].replace(\"V_\", \"\")\n",
    "        output[word] = pred\n",
    "        if pred == \"neg\":\n",
    "            negatives.append(word)\n",
    "    \n",
    "    #Assemble final output\n",
    "    final = {\n",
    "        \"negative_words\" : negatives,\n",
    "        \"polarities\"     : output\n",
    "    }\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative_words': ['overlap', 'removed', 'gear', 'cover', 'coolant', '\\n', '0305900srt0807e00', 'ran', 'truck', 'high', 'idle', 'freon', 'reservoir', 'radiator', 'srt010g900', '0305900srt0706z00', '010g900srt0706z00', '010g900steam', 'cleaned', 'engine', 'added', 'dye', 'leaking', 'hood', 'bumper', 'drained', 'recovered', 'ps', 'support', 'upper', 'lower', 'rad', 'hoses', 'ac', 'compressor', 'bracket', 'alternator', 'shroud', 'hub', 'resealed', 'reinstalled', 'all', 'refilled', 'leaks', 'repair', 'completeoil', 'leak', 'externalupper', 'gasketleaks', 'epr', 'number:430716600', 'oil1045962', 'thursday', '31oct2019', '05:00:47', 'am'], 'polarities': {',': 'pos', 'and': 'pos', 'overlap': 'neg', 'with': 'pos', 'removed': 'neg', 'gear': 'neg', 'cover': 'neg', 'coolant': 'neg', 'fan': 'pos', '\\n': 'neg', '0305900srt0807e00': 'neg', 'ran': 'neg', 'truck': 'neg', 'at': 'pos', 'high': 'neg', 'idle': 'neg', 'freon': 'neg', 'reservoir': 'neg', 'radiator': 'neg', 'srt010g900': 'neg', '0305900srt0706z00': 'neg', '010g900srt0706z00': 'neg', '010g900steam': 'neg', 'cleaned': 'neg', 'engine': 'neg', 'added': 'neg', 'dye': 'neg', 'found': 'pos', 'leaking': 'neg', 'hood': 'neg', 'bumper': 'neg', 'drained': 'neg', 'recovered': 'neg', 'ps': 'neg', 'both': 'pos', 'support': 'neg', 'upper': 'neg', 'lower': 'neg', 'rad': 'neg', 'hoses': 'neg', 'ac': 'neg', 'compressor': 'neg', 'bracket': 'neg', 'alternator': 'neg', 'shroud': 'neg', 'hub': 'neg', 'resealed': 'neg', 'reinstalled': 'neg', 'all': 'neg', 'parts': 'pos', 'refilled': 'neg', 'no': 'pos', 'leaks': 'neg', 'repair': 'neg', 'completeoil': 'neg', 'leak': 'neg', 'externalupper': 'neg', 'gasketleaks': 'neg', 'epr': 'neg', 'part': 'pos', 'number:430716600': 'neg', 'oil1045962': 'neg', 'thursday': 'neg', '31oct2019': 'neg', '05:00:47': 'neg', 'am': 'neg'}}\n"
     ]
    }
   ],
   "source": [
    "output = output_formatting(preds, internal)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Model to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('polarity.pk', 'wb') as fout:\n",
    "    pickle.dump(classifier, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Model from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('polarity.pk', 'rb') as fin:\n",
    "    polarity_nltk = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_loaded = polarity_nltk.classify_many(internal)\n",
    "preds == preds_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Function\n",
    "1. Tokenize\n",
    "2. Frequency distribution\n",
    "3. SL feature set\n",
    "4. Prep for inference\n",
    "5. Run model\n",
    "6. Map preds and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Dependencies\n",
    "SLpath = \"./subjclueslen1-HLTEMNLP05.tff\"\n",
    "SL = Subjectivity.readSubjectivity(SLpath)\n",
    "\n",
    "model_path = \"./polarity.pk\"\n",
    "with open(model_path, 'rb') as fin:\n",
    "    polarity_nltk = pickle.load(fin)\n",
    "\n",
    "def norma_polarity(params):\n",
    "    #Unpack Parameters\n",
    "    text = params[\"text\"].lower()\n",
    "    \n",
    "    #Tokenize Document\n",
    "    doc = tokenizer(text)\n",
    "    tokens = [word.text for word in doc]\n",
    "    \n",
    "    #Create Frequency Distribution\n",
    "    all_words = nltk.FreqDist(tokens)\n",
    "    word_items = all_words.most_common(5000)\n",
    "    word_features = [word for (word, count) in word_items]\n",
    "    \n",
    "    #SL Fecture Set\n",
    "    SL_featuresets = [SL_features(tokens, word_features, SL)]\n",
    "    \n",
    "    #Prep Feature for Inference\n",
    "    internal = prep_inference(SL_featuresets)[:-2]\n",
    "    \n",
    "    #Run Model\n",
    "    preds = polarity_nltk.classify_many(internal)\n",
    "    \n",
    "    #Map Predictions and Document Tokens\n",
    "    output = output_formatting(preds, internal)\n",
    "    \n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
